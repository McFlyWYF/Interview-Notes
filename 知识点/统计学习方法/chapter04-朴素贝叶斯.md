
# 朴素贝叶斯

### 基本方法

1．朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布
$P(X,Y)$，然后求得后验概率分布$P(Y|X)$。具体来说，利用训练数据学习$P(X|Y)$和$P(Y)$的估计，得到联合概率分布：

$$P(X,Y)＝P(Y)P(X|Y)$$

概率估计方法可以是极大似然估计或贝叶斯估计。

2．朴素贝叶斯法的基本假设是条件独立性，

$$\begin{aligned} P(X&=x | Y=c_{k} )=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}$$


这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。

### 后验概率最大化

* 朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测。

$$P(Y | X)=\frac{P(X, Y)}{P(X)}=\frac{P(Y) P(X | Y)}{\sum_{Y} P(Y) P(X | Y)}$$
 
将输入$x$分到后验概率最大的类$y$。

$$y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)$$

后验概率最大等价于0-1损失函数时的期望风险最小化。

* 后验概率最大化准则：
$$ f(x)=\arg \max _{c_k}P(c_k|X=x) $$
即朴素贝叶斯法所采用的原理。

模型：

- 高斯模型
- 多项式模型
- 伯努利模型

### 朴素贝叶斯的参数估计

#### 极大似然估计

* 先验概率$P(Y=c_k)$的极大似然估计是
$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, k=1,2,...,K
$$

* 条件概率$P(x^(j)=a_jl|Y=c_k)$的极大似然估计是
$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}
$$

$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值；$I$为指示函数。

#### 朴素贝叶斯算法

* 输入：训练数据T；
* 输出：实例x的分类；

（1）计算先验概率及条件概率
$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, k=1,2,...,K
$$

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}, j=1,2,...,n; l=1,2,...,S_j; k=1,2,...,K
$$

（2）对于给定的实例$x=(x^{(1)}, x^{(2)},..., x^{(n)})^T$，计算
$$
P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right), k=1,2,..,K
$$

（3）确定实例$x$的类
$$y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)$$

#### 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况，会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。

条件概率的贝叶斯估计是
$$
p_\lambda (X^{(j)}=\frac{a_{jl}|Y=c_k)=\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k) + S_j\lambda} 
$$

其中，$\lambda>=0$，当$\lambda=0$时，就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。

先验概率的贝叶斯估计是
$$
P_\lambda(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda}
$$

##### 优点

* 是生成模型，能处理多分类任务，适合增量式训练。
* 对缺失数据不敏感，算法比较简单，常用于文本分类。
