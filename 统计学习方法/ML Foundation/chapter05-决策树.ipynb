{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3d5728-8090-4b49-a4c0-e6bbe19d8928",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf30996-b2f9-4f98-bf68-72d35a6bb11b",
   "metadata": {},
   "source": [
    "决策树学习包括：特征选择，决策树生成（只考虑局部最优），决策树剪枝（考虑全局最优）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4db8e-0e9d-40aa-8fed-d5ea5eb42111",
   "metadata": {},
   "source": [
    "* 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc12957-784e-4610-a749-f1b42dabac6c",
   "metadata": {},
   "source": [
    "#### 决策树学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870224a-5b06-47a5-b612-7d0873304583",
   "metadata": {},
   "source": [
    "假设给定训练数据集\n",
    "$$\n",
    "D={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62c739-bec8-4725-9714-b6881cfeaeca",
   "metadata": {},
   "source": [
    "其中，$x_i=(x_i^(1),x_i^(2),...,x_i^(n))^T$为输入实例，$n$为特征个数，$y_i$为类标记，$N$为样本容量。学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04636a8-6b95-4d69-bd03-a8e511e435cf",
   "metadata": {},
   "source": [
    "决策树学习本质上是从训练数据集中归纳出一组分类规则。决策树学习是由训练数据集估计条件概率模型。决策树学习的损失函数通常是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50228d2c-4279-4f15-a9e3-ca2c3c8379d2",
   "metadata": {},
   "source": [
    "#### 决策树学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245afb6c-12f9-4db7-ac32-7acce18ebba3",
   "metadata": {},
   "source": [
    "通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e9f3f-2c38-4b8c-86ab-98b8cf8ba9ea",
   "metadata": {},
   "source": [
    "开始，构建根节点，将所有训练数据都放在根节点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点。如此递归的进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶节点上，即都有了明确的类，这就生成了一棵决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce174db-6206-44bf-9d73-4e21cfd0c64d",
   "metadata": {},
   "source": [
    "### 1.特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb0912-4745-4a3e-8b36-9d661a6acc72",
   "metadata": {},
   "source": [
    "#### 1.1  特征选择问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd99bd-4a40-4cfe-98f6-2179417eee0e",
   "metadata": {},
   "source": [
    "特征选择的准则是信息增益或信息增益比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8972e-c3b8-415f-96d8-e1f314a9ff10",
   "metadata": {},
   "source": [
    "#### 1.2 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6bf8d-f71f-446f-a7ea-0b0dbf99bac8",
   "metadata": {},
   "source": [
    "熵是表示随机变量不确定性的度量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f97e9b-58f6-45df-9071-988026deed94",
   "metadata": {},
   "source": [
    "X是一个随机变量，概率分布为$P(X=x_i)=p_i$，则随机变量X的熵定义为\n",
    "$$\n",
    "H(X)=-\\sum_{i=1}^{n}p_ilogp_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a01f75-fd18-4a55-9f93-e3fb464bdc6f",
   "metadata": {},
   "source": [
    "熵只依赖于X的分布，与X的取🈯值无关，也可以记作\n",
    "$$\n",
    "H(p)=-\\sum_{i=1}^{n}p_ilogp_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9543c5a-2bf4-4b67-9755-a46cebd99ecc",
   "metadata": {},
   "source": [
    "熵越大，随机变量的不确定性就越大。\n",
    "$$\n",
    "0\\le H(p)\\le logn\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00006fe-95c7-4cbf-b8f4-388be136ec2d",
   "metadata": {},
   "source": [
    "信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c949ac-8a9c-463c-b6c1-b96ab4ae0b57",
   "metadata": {},
   "source": [
    "* 特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52da2a7-b364-4fad-8d4c-9099ddc9881e",
   "metadata": {},
   "source": [
    "$H(D|A)$表示在特征A给定的条件下对数据集D进行分类的不确定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4545cfa-1c3d-4577-8b16-8f6ef6ec6052",
   "metadata": {},
   "source": [
    "一般熵H(Y)与条件熵H(Y|X)之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4151b8f-33f6-420e-bd95-c3d56213c193",
   "metadata": {},
   "source": [
    "##### 信息增益算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd35de-4af9-4f7b-a6d1-fa050e8071e3",
   "metadata": {},
   "source": [
    "* 输入：训练数据集D和特征A\n",
    "* 输出：特征A对训练数据集D的信息增益g(D,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f890fc3-6d2f-433e-9aca-d7ce0ffa79e7",
   "metadata": {},
   "source": [
    "(1) 计算数据集D的经验熵$H(D)$\n",
    "$$\n",
    "H(D)=-\\sum_{k=1}^{k}\\frac{|C_k|}{|D|}log_2 \\frac{|C_k|}{|D|} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc22810-6ee0-4108-a83e-f12b280e89a3",
   "metadata": {},
   "source": [
    "(2)计算特征A对数据集D的经验条件熵$H(D|A)$\n",
    "$$\n",
    "H(D|A)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{D_{ik}}{D_i} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67963df-aaa7-4ed0-aedb-4d290e8007f4",
   "metadata": {},
   "source": [
    "(3)计算信息增益\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc26d7f-127e-4b07-b765-334f7cc48dc7",
   "metadata": {},
   "source": [
    "#### 1.3信息增益比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420073ca-5356-47cb-84e5-5d0036a90613",
   "metadata": {},
   "source": [
    "使用信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行较正。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7a746-671f-4435-8f98-597ae920e64d",
   "metadata": {},
   "source": [
    "特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即\n",
    "$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562dfa53-19c9-431c-a7dc-a35ae7630135",
   "metadata": {},
   "source": [
    "其中，$H_A(D)=-\\sum_{i=1}^{n} \\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，n是特征A取值的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2233f0-88c7-4e40-9698-a84575e88b91",
   "metadata": {},
   "source": [
    "### 2.决策树的生成（ID3算法、C4.5算法）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf01148-a693-478f-9803-b0bdd642a09e",
   "metadata": {},
   "source": [
    "* ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e98e6-5e0a-4d94-8875-1d7940384f4a",
   "metadata": {},
   "source": [
    "* 从根节点出发，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。ID3相当于用极大似然法进行概率模型的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62bab6-6cec-4c23-a8b2-fee6c7211e7b",
   "metadata": {},
   "source": [
    "#### ID3算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01891d50-12bc-49a2-ac66-d4186d3e5944",
   "metadata": {},
   "source": [
    "* 输入：训练数据集D，特征集A，阈值$\\varepsilon$；\n",
    "* 输出：决策树T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a54c8-7154-43e8-8f28-6dc8ed2e11bc",
   "metadata": {},
   "source": [
    "（1）若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990652d6-b7ce-4d67-9633-3ac932a5b212",
   "metadata": {},
   "source": [
    "（2）若$A=\\phi$，则T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe011fb2-5650-46fe-94c9-0a71b2ef3068",
   "metadata": {},
   "source": [
    "（3）否则，按信息增益算法计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0368237-2eb1-4d78-8af4-306d4e43328e",
   "metadata": {},
   "source": [
    "（4）如果$A_g$的信息增益小于阈值$\\varepsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf397ae-3923-4573-8754-7c770cdbfea2",
   "metadata": {},
   "source": [
    "（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99683daf-665c-4d62-9670-5734cd9295ec",
   "metadata": {},
   "source": [
    "（6）对节点i，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用（1）～（5），得到子树$T_i$，返回$T_i$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f73c7-99dd-45b1-89e8-70e6359ceba9",
   "metadata": {},
   "source": [
    "**该算法生成的树容易产生过拟合。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde90f6e-23e3-4b15-b915-f7b5b6ac4cfd",
   "metadata": {},
   "source": [
    "#### C4.5算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afbab2-a776-47a4-ae06-dd306187cda2",
   "metadata": {},
   "source": [
    " C4.5算法在生成的过程中，用信息增益比来选择特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65328f-dfec-4f6e-9686-3bd1c8f9c49c",
   "metadata": {},
   "source": [
    "* 输入：训练数据集D，特征集A，阈值$\\varepsilon$；\n",
    "* 输出：决策树T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ceded-fef9-4966-b9c8-15446b253397",
   "metadata": {},
   "source": [
    "（1）若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd0968-667e-42b4-8240-49c765a613f8",
   "metadata": {},
   "source": [
    "（2）若$A=\\phi$，则T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a3951-eb79-47df-aa82-ee4986e57ca3",
   "metadata": {},
   "source": [
    "（3）否则，按信息增益比公式计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd33d19-8f12-46bd-9b39-7697ec483894",
   "metadata": {},
   "source": [
    "（4）如果$A_g$的信息增益比小于阈值$\\varepsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdb3ca-6018-4ed0-9a3d-edb514445b00",
   "metadata": {},
   "source": [
    "（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e83630-b208-4715-9eb3-db896445e882",
   "metadata": {},
   "source": [
    "（6）对节点i，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用（1）～（5），得到子树$T_i$，返回$T_i$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1675020-41ca-4b5a-9512-f05d1e3d334f",
   "metadata": {},
   "source": [
    "### 3.决策树的剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865fea0-bcb1-4cfe-a92b-b6f212e8bc87",
   "metadata": {},
   "source": [
    "决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e935b-0fa3-41de-b267-2f4b82d99222",
   "metadata": {},
   "source": [
    "决策树学习的损失函数定义为\n",
    "$$\n",
    "C_a(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha |T|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e7a76-0ad7-41b9-9ba5-748fb7f9968b",
   "metadata": {},
   "source": [
    "$T$为叶节点个数，$H_t(T)$为叶节点$t$上的经验熵，$N_t$是该叶节点的样本点。|T|表示模型复杂度，较大的$\\alpha$促使选择较简单的模型，较小的$\\alpha$促使选择较复杂的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646afe1-dd19-40d9-b208-a06c515131b3",
   "metadata": {},
   "source": [
    "经验熵为\n",
    "$$\n",
    "H_t(T)=-\\sum_{k}\\frac{N_{tk}}{N_t}log \\frac{N_{tk}}{N_t} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b51126-f69d-4a67-9f24-220d019d91a3",
   "metadata": {},
   "source": [
    "剪枝，就是当$\\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$\\alpha$确定时，子树越大，往往与训练数据的你和越好，但是模型的复杂度越高；反之则相反。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96299e64-e1fb-4c09-a46d-03d9382f08bd",
   "metadata": {},
   "source": [
    "损失函数的极小化等价于正则化的极大似然估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231089f4-239e-4236-a979-ac21727fec3b",
   "metadata": {},
   "source": [
    "#### 剪枝算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016af0ef-8b5e-44c8-9a94-633e5f4844a2",
   "metadata": {},
   "source": [
    "* 输入：生成算法产生的整个树T，参数$\\alpha$；\n",
    "* 输出：修剪后的子树$T_\\alpha$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694021d-fe32-419e-ab2d-e02ad0235f92",
   "metadata": {},
   "source": [
    "（1）计算每个结点的经验熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901877a3-48a7-4239-aa6b-af12d4778c01",
   "metadata": {},
   "source": [
    "（2）递归地从树的叶节点向上回缩。\n",
    "\n",
    "设一组叶节点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\\alpha(T_A)$与$C_\\alpha(T_B)$，如果\n",
    "$$\n",
    "C_\\alpha(T_A) \\le C_\\alpha(T_B)\n",
    "$$\n",
    "则进行剪枝，将父结点变为新的叶节点。也就是子树的损失函数值大于修剪之后的函数值，则对其进行剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37096227-0f07-4bd5-bdbb-3713eff0ffaf",
   "metadata": {},
   "source": [
    "（3）返回（2），直至不能继续为止，得到损失函数最小的子树$T_\\alpha$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc32c8-7075-499f-b5cf-a67e1d5f6822",
   "metadata": {},
   "source": [
    "### 4.CART算法（分类与回归树）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ff0ff-f87e-400d-8d3e-da71b48c8a9f",
   "metadata": {},
   "source": [
    "CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d01342-10d7-42d7-8cbc-866622c2239c",
   "metadata": {},
   "source": [
    "* CART算法由以下两步组成：\n",
    "    * （1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；\n",
    "    * （2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865c599-754b-4f59-9cbd-997cfc863d23",
   "metadata": {},
   "source": [
    "### 4.1CART生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2d94e-3cb4-4f68-a65d-d6234e5c0cbd",
   "metadata": {},
   "source": [
    "#### 回归树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e307535-3e5b-4d54-9cd1-613599561dca",
   "metadata": {},
   "source": [
    "##### 最小二乘回归树生成算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567099df-07a0-41b8-a066-dbc679d89566",
   "metadata": {},
   "source": [
    "* 输入：训练数据集D；\n",
    "* 输出：回归树$f(x)$；\n",
    "\n",
    "在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e793d-f4ec-4e93-819c-99c5689a2a51",
   "metadata": {},
   "source": [
    "（1） 选择最优切分变量j与切分点s，求解\n",
    "$$\n",
    "\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 +\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2 ] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6ad31-d0d4-4eed-ae1d-2f0e4ee8b7c4",
   "metadata": {},
   "source": [
    "遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到最小值的对$(j,s)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7acdbd-074c-41c8-a1ea-1c91c987ffbc",
   "metadata": {},
   "source": [
    "（2）用选定的对$(j,s)$划分区域并决定相应的输出值：\n",
    "$$\n",
    "R_1(j,s)={x|x^{(j)}\\le s}, R_2(j,s)={x|x^{(j)} > s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7489c-73eb-455a-bd84-d2bbe70a9e20",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{c_m}=\\frac{1}{N_m} \\sum_{x_i\\in R_m(j,s)}y_i,x\\in R_m,m=1,2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769eae35-0d3d-41b8-9a39-4131396d21a2",
   "metadata": {},
   "source": [
    "（3）继续对两个子区域调用步骤（1）（2），直至满足停止条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abee1f-7679-422e-b77c-3938ac21fd7f",
   "metadata": {},
   "source": [
    "（4）将输入空间划分为M个区域$R_1,R_2,...,R_M$，生成决策树：\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M}\\hat{c_m}I(x\\in R_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d374a-dded-4d14-99c5-e511bdf39bf6",
   "metadata": {},
   "source": [
    "#### 分类树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69edd3c1-b3f2-4346-a199-66ce05da4cad",
   "metadata": {},
   "source": [
    "分类树用基尼指数选择最优特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f785a9-02a7-4b02-a1db-26cd974885d4",
   "metadata": {},
   "source": [
    "##### 基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ce21f-038d-4345-b74d-09f414585b7a",
   "metadata": {},
   "source": [
    "* 分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为\n",
    "$$\n",
    "Gini(p)=\\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_k^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879dfce-1cf8-4667-b606-c50a16e46041",
   "metadata": {},
   "source": [
    "* 对于给定的样本集合D，其基尼指数为\n",
    "$$\n",
    "Gini(D)=1-\\sum_{k=1}^{K}(\\frac{|C_k|}{|D|})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724f893-4c70-4343-b666-b422645d5a62",
   "metadata": {},
   "source": [
    "$C_k$是D中属于第k类的样本子集，K是类的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ef9d9-3b7b-455a-bfc2-57a66a6c3080",
   "metadata": {},
   "source": [
    "* 特征A条件下，集合D的基尼指数定义为\n",
    "$$\n",
    "Gini(D,A)=\\frac{D_1}{D}Gini(D_1)+\\frac{D_2}{D}Gini(D_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724c8ff-68b1-4b40-864f-8cbe3ad0a36c",
   "metadata": {},
   "source": [
    "基尼指数$Gini(D)$表示集合D的不确定性，基尼指数$Gini(D,A)$表示经$A=\\alpha$分割后集合D的不确定性。基尼指数越大，样本集合的不确定性也越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8d78d-b90e-4f4d-9af5-bf0f01215155",
   "metadata": {},
   "source": [
    "##### CART生成算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5393e-3787-4313-abc0-5f291311f3d1",
   "metadata": {},
   "source": [
    "* 输入：训练数据集D，停止计算的条件；\n",
    "* 输出： CART决策树；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb337-9e7c-4ddb-8903-50b7d7e67508",
   "metadata": {},
   "source": [
    "（1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86c6ee-f3f9-441e-9af8-ffbac667cea0",
   "metadata": {},
   "source": [
    "（2）在所有可能的特征A以及它们所有可能的切分点$\\alpha$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，现结点生成两个子节点，将训练数据集依特征分配到两个子节点中去。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651eee2-753b-4dce-988b-f3904e7a682c",
   "metadata": {},
   "source": [
    "（3）对两个子节点递归地调用（1）（2），直至满足停止条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cf12f-bb8c-4eaa-bbcb-c53e927a11c4",
   "metadata": {},
   "source": [
    "（4）生成CART决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48445b-95eb-4c73-aa23-45f11d52a1f4",
   "metadata": {},
   "source": [
    "算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a79bc-3a7c-4f1e-88c5-7d15ded485a5",
   "metadata": {},
   "source": [
    "### 4.2 CART剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc9801-c9a5-4d95-8640-2148466f5c5c",
   "metadata": {},
   "source": [
    "* CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列${T_0,T_1,...,T_n}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154b805-5b3c-4a41-b760-a711934c3617",
   "metadata": {},
   "source": [
    "#### 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6261739-c6f6-425e-bec3-d9c41c66a9af",
   "metadata": {},
   "source": [
    "* 利用独立的验证数据集，测试子树序列$T_0,T_1,...,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3489a9-5455-44b9-9d4a-805b3686b2b0",
   "metadata": {},
   "source": [
    "* 输入：CART算法生成的决策树$T_0$；\n",
    "* 输出：最优决策树$T_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fca7ea-10c2-40a2-89ce-f9d69839b4bd",
   "metadata": {},
   "source": [
    "（1）设$k=0,T=T_0$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b70f15-c0a6-4c73-8b5f-e05a622f09d6",
   "metadata": {},
   "source": [
    "（2）设$\\alpha=+\\infty $；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a0e67-6d5d-4729-936a-18275b249028",
   "metadata": {},
   "source": [
    "（3）自下而上地对各内部结点t计算$C(T_t)，|T_t|$以及\n",
    "$$\n",
    "g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43059c6-b9df-428b-8bc8-da1463729fca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha=min(\\alpha,g(t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68645210-1adf-43ca-a6e3-d0a18e6fdbd0",
   "metadata": {},
   "source": [
    "$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422f453-3c59-4d8d-96e3-3f8c9a0c5154",
   "metadata": {},
   "source": [
    "（4）对$g(t)=\\alpha$的内部结点t进行剪枝，并对叶节点t以多数表决法决定其类，得到树T。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b43fb0-e102-4037-96e7-78bb389df338",
   "metadata": {},
   "source": [
    "（5）设$k=k+1,\\alpha_k=\\alpha,T_k=T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79b18f-1580-4b5f-b278-2922cec309c5",
   "metadata": {},
   "source": [
    "（6） 如果$T_k$不是由根节点及两个叶节点构成的树，则回到步骤3；否则令$T_k=T_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433276f-bd89-4494-aa7d-b778f78b9e5f",
   "metadata": {},
   "source": [
    "（7）采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\\alpha$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0f50c-9851-4113-9d33-d6c2289ac14b",
   "metadata": {},
   "source": [
    "#### 习题5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66a5715-abd5-4f60-8f0f-578fd2db6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "features = [\"年龄\", \"有工作\", \"有自己的房子\", \"信贷情况\"]\n",
    "X_train = pd.DataFrame([\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"是\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"好\"],\n",
    "    [\"中年\", \"是\", \"是\", \"好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"否\", \"一般\"]\n",
    "])\n",
    "y_train = pd.DataFrame([\"否\", \"否\", \"是\", \"是\", \"否\", \n",
    "                        \"否\", \"否\", \"是\", \"是\", \"是\", \n",
    "                        \"是\", \"是\", \"是\", \"是\", \"否\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4771bbb2-5b7a-4f71-ac33-c9ebad2c4c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.49.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"277pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 277.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 273,-310 273,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#bddef6\" stroke=\"black\" d=\"M218,-306C218,-306 107,-306 107,-306 101,-306 95,-300 95,-294 95,-294 95,-235 95,-235 95,-229 101,-223 107,-223 107,-223 218,-223 218,-223 224,-223 230,-229 230,-235 230,-235 230,-294 230,-294 230,-300 224,-306 218,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"103\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">有自己的房子 ≤ 3.0</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.48</text>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n",
       "<text text-anchor=\"start\" x=\"122\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 9]</text>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#f2c09c\" stroke=\"black\" d=\"M142,-187C142,-187 69,-187 69,-187 63,-187 57,-181 57,-175 57,-175 57,-116 57,-116 57,-110 63,-104 69,-104 69,-104 142,-104 142,-104 148,-104 154,-110 154,-116 154,-116 154,-175 154,-175 154,-181 148,-187 142,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"67\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">有工作 ≤ 3.0</text>\n",
       "<text text-anchor=\"start\" x=\"69\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"67.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"65\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 3]</text>\n",
       "<text text-anchor=\"start\" x=\"77.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.72,-222.91C138.52,-214.29 134.04,-205.09 129.7,-196.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.77,-194.48 125.24,-187.02 126.47,-197.54 132.77,-194.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"117.09\" y=\"-206.96\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M257,-179.5C257,-179.5 184,-179.5 184,-179.5 178,-179.5 172,-173.5 172,-167.5 172,-167.5 172,-123.5 172,-123.5 172,-117.5 178,-111.5 184,-111.5 184,-111.5 257,-111.5 257,-111.5 263,-111.5 269,-117.5 269,-123.5 269,-123.5 269,-167.5 269,-167.5 269,-173.5 263,-179.5 257,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"191.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"182.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"180\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 6]</text>\n",
       "<text text-anchor=\"start\" x=\"192.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.62,-222.91C188.09,-211.87 194.03,-199.9 199.54,-188.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.75,-190.18 204.06,-179.67 196.48,-187.07 202.75,-190.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.05\" y=\"-199.66\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M85,-68C85,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 85,0 85,0 91,0 97,-6 97,-12 97,-12 97,-56 97,-56 97,-62 91,-68 85,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"19.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"10.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"20.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 0</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84.28,-103.73C79.76,-95.06 74.99,-85.9 70.46,-77.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.56,-75.55 65.83,-68.3 67.35,-78.79 73.56,-75.55\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M200,-68C200,-68 127,-68 127,-68 121,-68 115,-62 115,-56 115,-56 115,-12 115,-12 115,-6 121,0 127,0 127,0 200,0 200,0 206,0 212,-6 212,-12 212,-12 212,-56 212,-56 212,-62 206,-68 200,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"123\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 3]</text>\n",
       "<text text-anchor=\"start\" x=\"135.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.1,-103.73C131.69,-95.06 136.54,-85.9 141.16,-77.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.27,-78.77 145.86,-68.3 138.09,-75.5 144.27,-78.77\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x103cc6d00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "le_x = preprocessing.LabelEncoder()\n",
    "le_x.fit(np.unique(X_train))\n",
    "X_train = X_train.apply(le_x.transform)\n",
    "\n",
    "le_y = preprocessing.LabelEncoder()\n",
    "le_y.fit(np.unique(y_train))\n",
    "y_train = y_train.apply(le_y.transform)\n",
    "\n",
    "# 调用sklearn.DT建立训练模型\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# 可视化\n",
    "dot_data = tree.export_graphviz(model_tree, out_file = None,\n",
    "                               feature_names = features,\n",
    "                               class_names = [str(k) for k in np.unique(y_train)],\n",
    "                               filled = True, rounded=True,\n",
    "                               special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7954e1a-22bb-4756-b6c8-42a8faf1b4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
