{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019fe329-4626-43a4-bfdb-2b6fcc83330e",
   "metadata": {},
   "source": [
    "# 提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ee3f3-1769-4806-8a50-46960a07ac4d",
   "metadata": {},
   "source": [
    "## 1.提升方法AdaBoost方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6f1f8-3349-48ed-848b-4fec5484606c",
   "metadata": {},
   "source": [
    "### 1.1 提升方法的基本思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b13520-e705-451d-a463-1322541aff57",
   "metadata": {},
   "source": [
    "在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。大多数的提升方法是改变训练数据的概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70440f58-3059-49a3-88cc-700a82ba8aef",
   "metadata": {},
   "source": [
    "存在两个问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8b79e-5754-4f7f-8c83-479f7a67ae1d",
   "metadata": {},
   "source": [
    "* 1.在每一轮如何改变训练数据的权值或概率分布；\n",
    "    * 提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类样本的权值。\n",
    "* 2.如何将弱分类器组合成一个强分类器；\n",
    "    * 采取加权多数表决的方法。加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04352eb3-831b-465c-9885-bd9d4d77f696",
   "metadata": {},
   "source": [
    "### 1.2 AdaBoost算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc27d19-852a-47ce-b8af-350b43ace7d7",
   "metadata": {},
   "source": [
    "可以认为是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19623960-5f2f-41b9-948a-114160dcbdc6",
   "metadata": {},
   "source": [
    "#### 步骤："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee8c0-e2bd-4c79-a34f-191426b05ca5",
   "metadata": {},
   "source": [
    "* 1）给每个训练样本分配权重，初始权重w；\n",
    "* 2）针对带有权值的样本进行训练，得到模型G；\n",
    "* 3）计算模型G的误分类率；\n",
    "* 4）计算模型G的系数；\n",
    "* 5）根据误分类率e和当前权重向量更新权重向量w；\n",
    "* 6）计算组合模型的误分类率；\n",
    "* 7）当组合模型的误分类率或迭代次数低于一定阈值，停止迭代；否则，回到2）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860b274-d4c1-4462-8f16-3a32ec40ece3",
   "metadata": {},
   "source": [
    "* 输入：训练数据集$T=\\left \\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\right \\} , y_i \\in Y=\\left \\{-1,+1\\right \\} $\n",
    "* 输出：最终分类器$G(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075d1e9-d4da-44a1-92c7-461d320d43ba",
   "metadata": {},
   "source": [
    "(1) 初始化训练数据的权值分布\n",
    "$$\n",
    "D_1=(w_{11},...,w_{1i},...,w_{1N}),w_{1i}=\\frac{1}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9468be-1310-4562-b967-52c2338726d7",
   "metadata": {},
   "source": [
    "(2) 对$m=1,2,...,M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251edb6d-de48-42c9-9e30-39439b13e41c",
   "metadata": {},
   "source": [
    "(a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n",
    "    $$\n",
    "    G_m(x):\\chi \\to \\left \\{-1,+1 \\right \\}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f81bd6-a4d7-4dfe-955e-78462388a648",
   "metadata": {},
   "source": [
    " (b) 计算$G_m(x)$在训练数据集上的分类误差率\n",
    "$$\n",
    "    e_m = \\sum_{i=1}^{N}P(G_m(x_i) \\ne y_i)=\\sum_{i=1}^{N}w_{mi}I(G_m(x_i) \\ne y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08139664-dd22-4953-9592-313a0e389517",
   "metadata": {},
   "source": [
    "(c) 计算$G_m(x)$系数\n",
    "$$\n",
    "a_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337b6bd-364d-4818-a5b4-b4f47edd95af",
   "metadata": {},
   "source": [
    "(d) 更新训练数据集的权值分布\n",
    "$$\n",
    "D_{m+1}=(w_{m+q,q},...,w_{m+1,i},...,w_{m+1,N})\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{m+1,i}=\\frac{w_{mi}}{z_m}exp(-\\alpha_my_iG_m(x_i))\n",
    "$$\n",
    "$Z_m$是规范化因子\n",
    "$$\n",
    "Z_m=\\sum_{i=1}^{N}w_{mi}exp(-\\alpha_my_iG_m(x_i)\n",
    "$$\n",
    "它使$D_{m+1}$成为一个概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a295382-a28f-46ec-9902-cc06778487d3",
   "metadata": {},
   "source": [
    "(3) 构建基本分类器的线性组合\n",
    "$$\n",
    "f(x)=\\sum_{m=1}{M}\\alpha_mG_m(x)\n",
    "$$\n",
    "得到最终分类器\n",
    "$$\n",
    "G(x)=sign(f(x))=sign(\\sum_{m=1}^{M}\\alpha_mG_m(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2944087-5da6-42e7-b2a4-ec17a7c7b5cb",
   "metadata": {},
   "source": [
    "## 2.AdaBoost算法的训练误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5cdc9-1ea5-49d5-84fa-0f6a87bb2d1e",
   "metadata": {},
   "source": [
    "##### 定理1：AdaBoost的训练误差界"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971a54a-ec54-4ee7-a3c3-36d21940587e",
   "metadata": {},
   "source": [
    "* AdaBoost算法最终分类器的训练误差界为\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}I(G(x_i)\\ne y_i)\\le \\frac{1}{N}\\sum_{i}exp(-y_if(x_i))= \\Pi_m Z_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbd7b9-4952-4782-8589-6a3f0192dcf0",
   "metadata": {},
   "source": [
    "在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eebfcb-b0d2-47ac-aad1-57adcdfc9125",
   "metadata": {},
   "source": [
    "##### 定理2：二类分类问题AdaBoost的训练误差界"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a608b-a40a-4c94-a6c5-7731993181d3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Pi_{m=1}^{M}Z_m=\\Pi_{m=1}^{M}[2\\sqrt{e_m(1-e+m)}]=\\Pi_{m=1}^{M}\\sqrt{(1-4\\gamma_m^2)}\\le exp(-2\\sum_{m=1}^{M}\\gamma_m^2)\n",
    "$$\n",
    "这里，$\\gamma_m=\\frac{1}{2}-e_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65255f-c715-4b04-ac55-da689764c074",
   "metadata": {},
   "source": [
    "如果存在$\\gamma > 0$，对所有$m$有$\\gamma_m \\ge \\gamma$，则\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}I(G(x_i)\\ne y_i)\\le exp(-2M\\gamma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694013cb-628f-4c8b-b137-d4d112ff9185",
   "metadata": {},
   "source": [
    "表明此条件下，AdaBoost的训练误差是以指数速率下降的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8832579-7f47-4423-a85a-05d8b3771199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
