{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e11714-2b3f-4a78-9285-38504a80078e",
   "metadata": {},
   "source": [
    "# EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09d498-3342-4588-83a5-2c00ce92bea0",
   "metadata": {},
   "source": [
    "* EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每步迭代由两步组成：E步，求期望，M步，求极大，所以称为期望极大算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8eb46-054e-4116-be3d-f6124fc66700",
   "metadata": {},
   "source": [
    "* EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d4050-075b-47ae-b250-4429014e1358",
   "metadata": {},
   "source": [
    "#### EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a25d2c-af8f-4117-a29b-c0ae6b83011b",
   "metadata": {},
   "source": [
    "* 输入：观测变量数据Y，隐变量数据Z，联合分布$P(Y<Z|\\theta)$，条件分布$P(Z|Y<\\theta)$\n",
    "* 输出：模型参数$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215f3e4-9e16-4ce4-8f4a-595db3d7befa",
   "metadata": {},
   "source": [
    "（1）选择参数的初值$\\theta^(0)$，开始迭代；\n",
    "\n",
    "（2）E步：记$\\theta^(i)$为第i次迭代参数$\\theta$的估计值，在第i+1次迭代的E步，计算\n",
    "\n",
    "$$\n",
    "Q(\\theta,\\theta^{(i)})=E_z[logP(Y,Z|\\theta)|Y,\\theta^{(i)}]\n",
    "=\\sum_{z}logP(Y,Z|\\theta)P(Z|Y,\\theta^{(i)})\n",
    "$$ **（Q函数）**\n",
    "\n",
    "（3）M步：求使$Q(\\theta, \\theta^{(i)})$极大化的$\\theta$，确定第i+1次迭代的参数的估计值$\\theta^{(i+1)}$\n",
    "\n",
    "$$\n",
    "\\theta^{(i+1)}=arg\\max_{\\theta} Q(\\theta,\\theta^{(i)})\n",
    "$$\n",
    "\n",
    "（4）重复第（2）步和第（3）步，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f411cce-8fb0-46bc-a9d5-106bf779f74e",
   "metadata": {},
   "source": [
    "* 1.参数的初值可以任意选择，但EM算法对初值是敏感的。\n",
    "* 2.$Q(\\theta,\\theta^{(i)})$的第1个变元表示要极大化的参数，第2个变院表示参数的当前估计值，每次迭代实际在求Q函数及其极大。\n",
    "* 3.M步求$Q(\\theta,\\theta^{(i)})$的极大化，得到$\\theta^{(i+1)}$，完成一次迭代，后面将证明每次迭代使似然函数增大或达到局部极值。\n",
    "* 4.给出停止迭代的条件，一般是对较小的整数$\\xi_1$，若满足\n",
    "\n",
    "$$\n",
    "||\\theta^{(i+1)}-\\theta^{(i)}||<\\xi_1\n",
    "$$\n",
    "\n",
    "则停止迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff1ea7-5e79-4f23-8b55-58bc5686ca5f",
   "metadata": {},
   "source": [
    "### EM算法的收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a0539-9e70-4763-aa0e-ccef619b5905",
   "metadata": {},
   "source": [
    "* 定理1：设$P(Y|\\theta)$为观测数据的似然函数，$\\theta^{(i)})$为EM算法得到的参数估计序列，$P(Y|\\theta^{(i)})$为对应的似然函数序列，则$P(Y|\\theta^{(i)})$是单调递增的，即\n",
    "\n",
    "$$\n",
    "P(Y|\\theta^{(i+1)})\\ge P(Y|\\theta^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3416e7e-6c00-42ad-8995-eb0372038a4d",
   "metadata": {},
   "source": [
    "* 定理2：设$L(\\theta)=logP(Y|\\theta)$为观测数据的对数似然函数，$\\theta^{(i)}$为EM算法得到的参数估计序列，$L(\\theta^{(i)})$为对应的对数似然函数序列。\n",
    "\n",
    "（1）如果$P(Y|\\theta)$有上界，则$L(\\theta^{(i)})=logP(Y|\\theta^{(i)})$收敛到某一值$L^*$；\n",
    "\n",
    "（2）在函数$Q(\\theta,\\theta^{'})$与$L(\\theta)$满足一定条件下，由EM算法得到的参数估计序列$\\theta^{(i)}$的收敛值$\\theta_*$是$L(\\theta)$的稳定点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edc274-10e7-4f4f-9cb6-c01cee6839b0",
   "metadata": {},
   "source": [
    "### EM算法在高斯混合模型学习中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e365ebe-0a1f-4c05-9d58-9ccbce826689",
   "metadata": {},
   "source": [
    "#### 高斯混合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11677363-1e19-413f-91db-66ad798ad6a0",
   "metadata": {},
   "source": [
    "* 定义：高斯混合模型是指具有如下形式的概率分布模型：\n",
    "\n",
    "$$\n",
    "P(y|\\theta)=\\sum_{k=1}^{K}\\alpha_k \\phi(y|\\theta_k)\n",
    "$$\n",
    "\n",
    "$\\alpha_k$是系数，$\\phi(y|\\theta_k)$是高斯分布密度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbb5c4-7102-4d58-b51b-5abf8f8f87d6",
   "metadata": {},
   "source": [
    "#### 高斯混合模型参数估计的EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa9828-fc70-49a8-8992-74f62dcee725",
   "metadata": {},
   "source": [
    "* 输入：观测数据$y_1,y_2,...,y_N$，高斯混合模型；\n",
    "* 输出：高斯混合模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873cfd0-2719-4d40-a09f-8bd72068e37c",
   "metadata": {},
   "source": [
    "（1）取参数的初始值开始迭代；\n",
    "\n",
    "（2）E步：依据当前模型参数，计算分模型k对观测数据$y_i$的响应度\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma_{jk}}=\\frac{\\alpha_k \\phi (y_j|\\theta_k)}{\\sum_{k=1}^{K} \\alpha_k \\phi(y_j|\\theta_k)}\n",
    "$$\n",
    "\n",
    "（3）M步：计算新一轮迭代的模型参数\n",
    "\n",
    "$$\n",
    "\\hat{\\mu_k}=\\frac{\\sum_{j=1}^{N}\\hat{\\gamma_{jk}}y_j}{\\sum_{j=1}^{N}\\hat{\\gamma_{jk}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma_k^2}=\\frac{\\sum_{j=1}^{N}\\hat{\\gamma_{jk}(y_j-\\mu_k)^2}y_j}{\\sum_{j=1}^{N}\\hat{\\gamma_{jk}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha_k}=\\frac{\\sum_{j=1}^{N}\\hat{\\gamma_{jk}}}{N}\n",
    "$$\n",
    "\n",
    "（4）重复第（2）步和第（3）步，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712ee7f-8dcb-4f6a-b5be-5c1cd93fba85",
   "metadata": {},
   "source": [
    "### EM算法的推广"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517a8da-3f11-4c48-97fc-37dae1ac0ae6",
   "metadata": {},
   "source": [
    "#### 1.F函数的极大-极大算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7d705-4219-49d1-8fdd-822294381ff7",
   "metadata": {},
   "source": [
    "#### 2.GEM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db82db0-afbe-4a5c-8b0f-4c0914262f88",
   "metadata": {},
   "source": [
    "* 每次迭代增加F函数值，从而增加似然函数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c16e64-648b-4083-90bb-401b96ac72a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
