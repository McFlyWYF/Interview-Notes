{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d4cd0a-a3f1-4a0f-8816-3f8ee1d72459",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c06079-e0dc-42fc-9d55-94b065d465f9",
   "metadata": {},
   "source": [
    "### 1.决策树的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898b3f9-d190-4d7a-96ee-357744f8c4eb",
   "metadata": {},
   "source": [
    "* 决策树是一种树结构，从根节点出发，每个分支都将训练数据划分成了互不相交的子集。分支的划分可以以单个特征为依据，也可以以特征的线性组合为依据。决策树可以解决回归和分类问题，在预测过程中，一个测试数据会依据已经训练好的决策树到达某一叶子节点，该叶子节点即为回归或分类问题的预测结果。\n",
    "\n",
    "* 从概率论的角度理解，决策树是定义在特征空间和类空间上的条件概率分布。每个父节点可以看作子树的先验分布，子树则为父节点在当前特征划分下的后验分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ef9ef-daad-4953-8999-efbf93fdaa75",
   "metadata": {},
   "source": [
    "### 2. 信息增益和信息增益率的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707d9b7-1444-48c3-bce4-86873e918219",
   "metadata": {},
   "source": [
    "* 1）信息熵：信息熵用来度量样本集合的纯度，信息熵值越小，D 的纯度越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373da70-2c61-4c40-9e10-6f63c361daf2",
   "metadata": {},
   "source": [
    "* 2）信息增益：信息增益用来描述一次划分之后纯度的提升有多大。用不同的属性划分样本，会得到不同的信息增益。在 ID3 决策树算法中，我们取能使信息增益最大，即划分后纯度提升最大的属性作为当前决策树的划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628caa34-1218-48d9-b653-0e6ae4851aab",
   "metadata": {},
   "source": [
    "* 3）信息增益率：使用信息增益当作 cost function 会对可取值数目较多的属性有所偏好，使用信息增益率可以减小这种偏好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a16d7-6ab1-43b3-b4d3-fddb6c35cf8f",
   "metadata": {},
   "source": [
    "### 3.决策树出现过拟合的原因及解决办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8c66b-fd37-40c3-a550-ffd53fbcfaa7",
   "metadata": {},
   "source": [
    "* 原因：\n",
    "    * 在决策树构建的过程中，对决策树的生长没有进行合理的限制（剪枝）；\n",
    "    * 样本中有一些噪声数据，没有对噪声数据进行有效的剔除；\n",
    "\n",
    "* 解决办法\n",
    "    * 选择合理的参数进行剪枝，可以分为预剪枝和后剪枝，我们一般采用后剪枝的方法；\n",
    "    * 利用K-folds交叉验证，将训练集分为K份，然后进行K次交叉验证，每次使用K-1份作为训练样本数据集，另外一份作为测试集；\n",
    "    * 减少特征，计算每一个特征和响应变量的相关性，常见得为皮尔逊相关系数，将相关性较小的变量剔除；当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等（决策的正则化，例如，L1和L2正则，具体是对谁的正则呢？怎样正则的呢？）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374fecf-d6fd-40a8-8172-4cf6d53e57c8",
   "metadata": {},
   "source": [
    "### 4.如何对决策树进行剪枝？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba07d8-aae9-4fba-912a-c7dc0e94165d",
   "metadata": {},
   "source": [
    "剪枝是防止决策树过拟合的方法。一棵完全生长的决策树很可能失去泛化能力，因此需要剪枝。\n",
    "\n",
    "* 1）剪枝的策略\n",
    "    * 剪枝分为预剪枝和后剪枝两种，预剪枝是在构建决策树时抑制它的生长，后剪枝是决策树生长完全后再对叶子节点进行修剪。\n",
    "\n",
    "* 2）预剪枝\n",
    "    * 对于一个决策树，每次生长前，可以判断生长后系统在验证集上准确度是否提升，如果经过一次生长，系统在验证集上的准确度降低了，那么中止这次生长。\n",
    "        * 设置一个树的最大高度/深度或者为树设置一个最大节点数，达到这个值即停止生长\n",
    "        * 对每个叶子节点的样本数设置最小值，生长时叶子节点样本数不能小于这个值\n",
    "        * 判断每次生长对系统性能是否有增益\n",
    "\n",
    "* 3）后剪枝\n",
    "\n",
    "    * 后剪枝方法是对一棵已经完全生长的决策树进行剪枝。错误率降低剪枝的方法比较直观，从下至上遍历所有非叶子节点的子树，每次把子树剪枝（所有数据归到该节点，将数据中最多的类设为结果），与之前的树在验证集上的准确率进行比较，如果有提高，则剪枝，否则不剪，直到所有非叶子节点被遍历完。\n",
    "    * 错误率降低剪枝（Reduced-Error Pruning）\n",
    "    * 悲观剪枝（Pessimistic Error Pruning）\n",
    "    * 代价复杂度剪枝（Cost-Complexity Pruning）\n",
    "\n",
    "* 4）预剪枝和后剪枝的优缺点比较\n",
    "    * 时间成本方面，预剪枝在训练过程中即进行剪枝，后剪枝要在决策树完全生长后自底向上逐一考察。显然，后剪枝训练时间更长。预剪枝更适合解决大规模问题。\n",
    "    * 剪枝的效果上，预剪枝的常用方法本质上是基于贪心的思想，但贪心法却可能导致欠拟合，后剪枝的欠拟合风险很小，泛化性能更高。\n",
    "    * 另外，预剪枝的有些方法使用了阈值，如何设置一个合理的阈值也是一项挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80030950-8950-4cf5-b981-888ecc6e0505",
   "metadata": {},
   "source": [
    "### 5.决策树需要进行归一化处理吗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8ec6d-3dab-4114-a543-ae6565ff5f88",
   "metadata": {},
   "source": [
    "* 概率模型不需要归一化，因为他们不关心变量的值，而是关心变量的分布和变量之间的条件概率。决策树是一种概率模型，数值缩放，不影响分裂点位置。所以一般不对其进行归一化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d23ab-bcab-419a-94cb-e85d1fad724e",
   "metadata": {},
   "source": [
    "### 6.决策树如何处理缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532f5c5-2a77-48d7-a376-d02fcd73594b",
   "metadata": {},
   "source": [
    "* ①抛弃缺失值\n",
    "* ②补充缺失值\n",
    "* ③概率化缺失值，对缺失值的样本赋予该属性所有属性值的概率分布。\n",
    "* ④缺失值单独分支"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc58b0b-598e-4ab3-b5ff-6cede7cc6ecf",
   "metadata": {},
   "source": [
    "### 7. 决策树与逻辑回归的区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7c0ec-d18a-4115-96a8-a75fc0bd6da8",
   "metadata": {},
   "source": [
    "* 逻辑回归通常用于分类问题，决策树可回归、可分类。\n",
    "* 逻辑回归是线性函数，决策树是非线性函数（没有方程式可以表达自变量和因变量之间的关系）。\n",
    "* 逻辑回归的核心是sigmoid函数，具有无限可导的优点，常作为神经网络的激活函数。\n",
    "* 逻辑回归较为简单，不容易产生过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ff17b-92ac-4eae-be52-292978be0250",
   "metadata": {},
   "source": [
    "* 对于拥有缺失值的数据，决策树可以应对，而逻辑回归需要挖掘人员预先对缺失数据进行处理；\n",
    "\n",
    "* 逻辑回归对数据整体结构的分析优于决策树，而决策树对局部结构的分析优于逻辑回归；（决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。）\n",
    "\n",
    "* 逻辑回归擅长分析线性关系，而决策树对线性关系的把握较差。线性关系在实践中有很多优点：简洁，易理解，可以在一定程度上防止对数据的过度拟合。（我自己对线性的理解：1，逻辑回归应用的是样本数据线性可分的场景，输出结果是概率，即，输出结果和样本数据之间不存在直接的线性关系；2，线性回归应用的是样本数据和输出结果之间存在线性关系的场景，即，自变量和因变量之间存在线性关系。）\n",
    "\n",
    "* 逻辑回归对极值比较敏感，容易受极端值的影响，而决策树在这方面表现较好。\n",
    "\n",
    "* 应用上的区别：决策树的结果和逻辑回归相比略显粗糙。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。比如决策树确定17个节点，全部数据就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要去一定的训练和技巧。\n",
    "\n",
    "* 执行速度上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6921dd-4bdf-4cd9-b090-e02b19fa0eaf",
   "metadata": {},
   "source": [
    "### 8.决策树的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b87a30-95f6-44cf-842a-7ee2450b547c",
   "metadata": {},
   "source": [
    "* 优点\n",
    "    * 决策树可以可视化，易于理解和解释；\n",
    "    * 数据准备工作很少。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等；\n",
    "    * 能够同时处理数值和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类型的数据集；\n",
    "    * 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度；\n",
    "    * 能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开；\n",
    "    * 是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca860314-7462-428c-9632-958b83136939",
   "metadata": {},
   "source": [
    "* 缺点\n",
    "    * 递归生成树的方法很容易出现过拟合。\n",
    "    * 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树；\n",
    "    * 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在拟合决策树之前平衡数据集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
